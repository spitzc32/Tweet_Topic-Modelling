{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the Classifier/Filter</h2>\n",
    "Currently we are trying to train the whole classifier with a single function, the train function. Next is we will call the pipeline to filter and  clean all the tweets we need to clean. After this, we will perform tweet cleaning\n",
    "\n",
    "<h4>Tweet Cleaning</h4>\n",
    "This is where the real operation starts. We will filter the tweets based on the saved model, and clean them in a proper manner. The steps are as follows:\n",
    "\n",
    "* Filter Tweets (filter_pipeline)\n",
    "* Clean Tweets (data_pipeline)\n",
    "* format Tweets to pass in our model (data_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.7883597883597884\n",
      "\n",
      "Classification Report for\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.83       434\n",
      "           1       0.80      0.67      0.73       322\n",
      "\n",
      "    accuracy                           0.79       756\n",
      "   macro avg       0.79      0.77      0.78       756\n",
      "weighted avg       0.79      0.79      0.79       756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train.classifier import train\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Topic Segmentation</h2>\n",
    "This is where we segment tweets based on the frequency of the word that appears on the document, therefore classifying that word based on the frequency output.\n",
    "\n",
    "We will perform the following\n",
    "\n",
    "*  term frequencyâ€“inverse document frequency(TF-IDF) to transpose the words into vectors of frequency\n",
    "* 3 models to test\n",
    "    * Logistic Regression\n",
    "    * SGD Classfier\n",
    "    * SGD Modified Huber\n",
    "* Check the accuracy metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13777\n",
      "\n",
      " Accuracy:  0.9412191582002902\n",
      "\n",
      "Classification Report for\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       201\n",
      "           1       0.98      0.93      0.95       191\n",
      "           2       0.99      0.99      0.99       189\n",
      "           3       0.94      0.91      0.92       181\n",
      "           4       0.89      0.90      0.90       226\n",
      "           5       0.96      0.98      0.97       195\n",
      "           6       0.85      0.92      0.88       195\n",
      "\n",
      "    accuracy                           0.94      1378\n",
      "   macro avg       0.94      0.94      0.94      1378\n",
      "weighted avg       0.94      0.94      0.94      1378\n",
      "\n",
      "\n",
      " Accuracy:  0.9412191582002902\n",
      "\n",
      "Classification Report for\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       201\n",
      "           1       0.98      0.93      0.96       191\n",
      "           2       0.99      0.99      0.99       189\n",
      "           3       0.92      0.91      0.91       181\n",
      "           4       0.93      0.88      0.90       226\n",
      "           5       0.96      0.98      0.97       195\n",
      "           6       0.83      0.93      0.88       195\n",
      "\n",
      "    accuracy                           0.94      1378\n",
      "   macro avg       0.94      0.94      0.94      1378\n",
      "weighted avg       0.94      0.94      0.94      1378\n",
      "\n",
      "\n",
      " Accuracy:  0.137155297532656\n",
      "\n",
      "Classification Report for\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       201\n",
      "           1       0.00      0.00      0.00       191\n",
      "           2       0.14      1.00      0.24       189\n",
      "           3       0.00      0.00      0.00       181\n",
      "           4       0.00      0.00      0.00       226\n",
      "           5       0.00      0.00      0.00       195\n",
      "           6       0.00      0.00      0.00       195\n",
      "\n",
      "    accuracy                           0.14      1378\n",
      "   macro avg       0.02      0.14      0.03      1378\n",
      "weighted avg       0.02      0.14      0.03      1378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jayra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jayra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from train.frequency import train as train_freq\n",
    "\n",
    "train_freq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Filipino Vocabulary</h4>\n",
    "Testing English vocab model to Taglish Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13816\n"
     ]
    }
   ],
   "source": [
    "from train.frequency import test_accuracy\n",
    "\n",
    "test_accuracy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results</h4>\n",
    "We can see that our results tends to favour the Frequency distribution since our data is controlled, we have set of keywords related to each other so some keywords may be present to the other keyworded tweet, which brought about the good results since our feature vector tends to see this similarity.\n",
    "\n",
    "<h4>Error Analysis</h4>\n",
    "We had a few errors before getting about a good result, we divide them according to the process done\n",
    "\n",
    "Preprocessing\n",
    "* Lack of Handling the appropriate stopwords for the dataset\n",
    "* Spellchecker\n",
    "* Appropriate Root Word stem\n",
    "* make tweets unique\n",
    "\n",
    "Feature Vector (LDA)\n",
    "* Additional N time for transposing and transforming Cohesion Pair\n",
    "* took 30 mins to train 13,890 unique tweets\n",
    "\n",
    "Feature Vector (TF-IDF)\n",
    "* Less transposing done by reshaping the features into the same Y dim\n",
    "* Used l2 regularization to lessen the complexity of the model\n",
    "* Used bounded frequency to lessen the occurence of unhandled stop words\n",
    "\n",
    "Model (Disaster of Not)\n",
    "* Best model that has a score of 0.7883597883597884, is SVM\n",
    "* Out of 5 models, worst result kNN with a score of 0.525\n",
    "\n",
    "Model (Topic Segmentations)\n",
    "* Best model that has a score of 0.9412191582002902, is tie to both Logistic Regression and SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.frequency import test_accuracy\n",
    "with open(\"./saved_models/SGDHuberFreq.pkl\", 'rb') as file:\n",
    "    freq_model = pickle.load(file)\n",
    "\n",
    "print(type(freq_model))\n",
    "X_testset = Cleaner(\n",
    "        ds_train=df_tweets_en,\n",
    "        column=\"rawContent\"\n",
    "    ).fit_testset()\n",
    "\n",
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1,2,3,4,5,6])\n",
    "print(arr.shape)\n",
    "arr = arr.reshape(3,2)\n",
    "print(arr)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83d04032e859d7f6b4884d0eaea2daded37857bd69838437e04f68722128dc82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
